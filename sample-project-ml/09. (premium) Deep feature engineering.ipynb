{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 9: Deep feature engineering\n",
    "\n",
    "Before the beginning of this stage, we have selected ML models and some custom features in the dataset. Now we have an opportunity to improve the accuracy of the model significantly, by adding more sophisticated features to the dataset and applying feature selection methods.\n",
    "\n",
    "Stages of the premium project part are usually tailored to each specific task. Because of this, we can only describe the general methods what we typically do. \n",
    "\n",
    "\n",
    "### Mining new features\n",
    "\n",
    "The goal of this stage is to find 'killer features' for the dataset which significantly improve the performance of the machine-learning models. These features might be:\n",
    "\n",
    "- Finding extra data. For example, fetching Caseâ€“Shiller index data for home prices dataset and building an extra set of features for it. https://en.wikipedia.org/wiki/Case%E2%80%93Shiller_index\n",
    "\n",
    "\n",
    "- Mining extra information from dataset features. For example, if the dataset contains people names with title 'Mr. John Smith', 'Joe Doe PhD.', we can extract information about the gender, education, and social status.\n",
    "\n",
    "\n",
    "- Information integration - sometimes the dataset contains information about the different aspects of the object (the house in our case). Sometimes it's possible to integrate various features to get more information about the object. For example, we have the home 'Quality' feature and the 'SquareFeet' feature so that we can use 'Quality' * 'SquareFeet' as a quantitative metric of the overall quality and look from the different angle on these features. \n",
    "\n",
    "### Features selection and dimensionality reduction\n",
    "Typically the general rule of the machine-learning is to collect as much data as possible. But sometimes, too many features might become a problem, some of those features are noisy and don't describe the dataset correctly. \n",
    "\n",
    "Some machine-learning models can suffer from the curse of the dimensionality (https://en.wikipedia.org/wiki/Curse_of_dimensionality), and because of this, we must implement dimensionality reduction techniques, such as PCA or LDA. \n",
    "\n",
    "The feature selection methods can significantly increase the performance of the ML models, typically we use:\n",
    "* Sequential feature selections methods (http://research.cs.tamu.edu/prism/lectures/pr/pr_l11.pdf)\n",
    "* A balanced iterative random forest feature selection method (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3766035/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
